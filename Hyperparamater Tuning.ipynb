{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def objective_logistic_regression(trial):\n",
    "    # Suggest hyperparameters\n",
    "    penalty = trial.suggest_categorical(\"penalty\", [None, \"l2\"])  # Regularization type\n",
    "    max_iter = trial.suggest_int(\"max_iter\", 200, 1000, step=50)  # Number of iterations\n",
    "\n",
    "    # Train Logistic Regression model\n",
    "    model = LogisticRegression(penalty=penalty, max_iter=max_iter, random_state=1)\n",
    "\n",
    "    # Flatten input features\n",
    "    X_train_flattened = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test_flattened = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "    # Fit model\n",
    "    model.fit(X_train_flattened, y_train)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred = model.predict(X_test_flattened)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy  # Optuna will maximize this\n",
    "\n",
    "# Run Optuna optimization\n",
    "study_logistic = optuna.create_study(direction=\"maximize\")  # We want to maximize accuracy\n",
    "study_logistic.optimize(objective_logistic_regression, n_trials=50)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Best hyperparameters:\", study_logistic.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FCNN tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)  # Number of hidden layers\n",
    "    layer_sizes = [trial.suggest_int(f\"n_units_l{i}\", 16, 256, step=16) for i in range(n_layers)]\n",
    "    alpha = trial.suggest_float(\"alpha\", 1e-5, 1e-1, log=True)  # L2 regularization\n",
    "    max_iter = trial.suggest_int(\"max_iter\", 200, 500, step=50)  # Number of iterations\n",
    "\n",
    "    # Flatten input features\n",
    "    X_train_flattened = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test_flattened = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "    # Create MLP model with dynamic architecture\n",
    "    model = MLPClassifier(hidden_layer_sizes=tuple(layer_sizes), \n",
    "                          alpha=alpha, \n",
    "                          max_iter=max_iter, \n",
    "                          random_state=1)\n",
    "\n",
    "    # Train model\n",
    "    model.fit(X_train_flattened, y_train)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred = model.predict(X_test_flattened)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy  # Optuna will maximize this\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction=\"maximize\")  # We want to maximize accuracy\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TNN Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def objective(trial, X_train, X_test, X_val, y_train, y_test, y_val, num_of_classes, epochs, patience=50):\n",
    "    # Suggest hyperparameters\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
    "    \n",
    "    train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "    val_dataset = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
    "\n",
    "    trainloader = DataLoader(train_dataset, shuffle=True, batch_size=256)\n",
    "    valloader = DataLoader(val_dataset, shuffle=False, batch_size=256)\n",
    "    \n",
    "    model = TensorNet(X_train[0].shape, num_of_classes=num_of_classes)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    best_val_loss = 999\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    # Training loop with validation\n",
    "    for epoch in range(epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for windows, labels in trainloader:\n",
    "            predictions = model(windows)\n",
    "            loss = criterion(predictions, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Compute validation accuracy\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for windows, labels in valloader:\n",
    "                predictions = model(windows)\n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(valloader)\n",
    "        \n",
    "        #if epoch % 10 == 0:\n",
    "            # print(f'Epoch [{epoch}/{epochs}], Loss: {total_loss/len(trainloader):.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Check if the current model is the best\n",
    "        if val_loss < best_val_loss or best_val_loss is None:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()  # Save the best model state\n",
    "            patience_counter = 0  # Reset counter if loss improves\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}, best val loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "    \n",
    "    # Load the best model before testing\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Test the model\n",
    "    test_dataset = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "    testloader = DataLoader(test_dataset, shuffle=False, batch_size=256)\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for windows, labels in testloader:\n",
    "            predictions = model(windows)\n",
    "            _, predicted = torch.max(predictions.data, 1)\n",
    "            _, ground_truth = torch.max(labels.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == ground_truth).sum().item()\n",
    "\n",
    "    accuracy_test = correct / total\n",
    "    print(f'Test Accuracy: {100 * accuracy_test:.2f}%')\n",
    "    \n",
    "    return accuracy_test\n",
    "\n",
    "def tune_tnn(X_train, X_test, X_val, y_train, y_test, y_val, num_of_classes, epochs, n_trials=50):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective(trial, X_train, X_test, X_val, y_train, y_test, y_val, num_of_classes, epochs), n_trials=n_trials)\n",
    "    \n",
    "    print(\"Best hyperparameters:\", study.best_params)\n",
    "    return study.best_params\n",
    "\n",
    "\n",
    "best_params = tune_tnn(X_train, X_test, X_val, y_train, y_test, y_val, 16, 1000, n_trials=50)\n",
    "print(\"Optimized Hyperparameters:\", best_params)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
